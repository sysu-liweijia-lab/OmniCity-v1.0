{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd038b6f4c079b4d78d4dfbb44d1e011c60cf53225d3539ab35a3f320732d3fcbed",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os.path as osp\n",
    "import pickle\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import mmcv\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from mmcv.image import tensor2imgs\n",
    "from mmcv.runner import get_dist_info\n",
    "\n",
    "from mmdet.core import encode_mask_results\n",
    "\n",
    "\n",
    "def single_gpu_test(model,\n",
    "                    data_loader,\n",
    "                    show=False,\n",
    "                    out_dir=None,\n",
    "                    show_score_thr=0.3):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    dataset = data_loader.dataset\n",
    "    prog_bar = mmcv.ProgressBar(len(dataset))\n",
    "    for i, data in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            result = model(return_loss=False, rescale=True, **data)\n",
    "\n",
    "        batch_size = len(result)\n",
    "        if show or out_dir:\n",
    "            if batch_size == 1 and isinstance(data['img'][0], torch.Tensor):\n",
    "                img_tensor = data['img'][0]\n",
    "            else:\n",
    "                img_tensor = data['img'][0].data[0]\n",
    "            img_metas = data['img_metas'][0].data[0]\n",
    "            imgs = tensor2imgs(img_tensor, **img_metas[0]['img_norm_cfg'])\n",
    "            assert len(imgs) == len(img_metas)\n",
    "\n",
    "            for i, (img, img_meta) in enumerate(zip(imgs, img_metas)):\n",
    "                h, w, _ = img_meta['img_shape']\n",
    "                img_show = img[:h, :w, :]\n",
    "\n",
    "                ori_h, ori_w = img_meta['ori_shape'][:-1]\n",
    "                img_show = mmcv.imresize(img_show, (ori_w, ori_h))\n",
    "\n",
    "                if out_dir:\n",
    "                    out_file = osp.join(out_dir, img_meta['ori_filename'])\n",
    "                else:\n",
    "                    out_file = None\n",
    "\n",
    "                model.module.show_result(\n",
    "                    img_show,\n",
    "                    result[i],\n",
    "                    show=show,\n",
    "                    out_file=out_file,\n",
    "                    score_thr=show_score_thr)\n",
    "\n",
    "        # encode mask results\n",
    "        if isinstance(result[0], tuple):\n",
    "            result = [(bbox_results, encode_mask_results(mask_results))\n",
    "                      for bbox_results, mask_results in result]\n",
    "        results.extend(result)\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            prog_bar.update()\n",
    "    return results\n",
    "\n",
    "\n",
    "def multi_gpu_test(model, data_loader, tmpdir=None, gpu_collect=False):\n",
    "    \"\"\"Test model with multiple gpus.\n",
    "\n",
    "    This method tests model with multiple gpus and collects the results\n",
    "    under two different modes: gpu and cpu modes. By setting 'gpu_collect=True'\n",
    "    it encodes results to gpu tensors and use gpu communication for results\n",
    "    collection. On cpu mode it saves the results on different gpus to 'tmpdir'\n",
    "    and collects them by the rank 0 worker.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to be tested.\n",
    "        data_loader (nn.Dataloader): Pytorch data loader.\n",
    "        tmpdir (str): Path of directory to save the temporary results from\n",
    "            different gpus under cpu mode.\n",
    "        gpu_collect (bool): Option to use either gpu or cpu to collect results.\n",
    "\n",
    "    Returns:\n",
    "        list: The prediction results.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    dataset = data_loader.dataset\n",
    "    rank, world_size = get_dist_info()\n",
    "    if rank == 0:\n",
    "        prog_bar = mmcv.ProgressBar(len(dataset))\n",
    "    time.sleep(2)  # This line can prevent deadlock problem in some cases.\n",
    "    for i, data in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            result = model(return_loss=False, rescale=True, **data)\n",
    "            # encode mask results\n",
    "            if isinstance(result[0], tuple):\n",
    "                result = [(bbox_results, encode_mask_results(mask_results))\n",
    "                          for bbox_results, mask_results in result]\n",
    "        results.extend(result)\n",
    "\n",
    "        if rank == 0:\n",
    "            batch_size = len(result)\n",
    "            for _ in range(batch_size * world_size):\n",
    "                prog_bar.update()\n",
    "\n",
    "    # collect results from all ranks\n",
    "    if gpu_collect:\n",
    "        results = collect_results_gpu(results, len(dataset))\n",
    "    else:\n",
    "        results = collect_results_cpu(results, len(dataset), tmpdir)\n",
    "    return results\n",
    "\n",
    "\n",
    "def collect_results_cpu(result_part, size, tmpdir=None):\n",
    "    rank, world_size = get_dist_info()\n",
    "    # create a tmp dir if it is not specified\n",
    "    if tmpdir is None:\n",
    "        MAX_LEN = 512\n",
    "        # 32 is whitespace\n",
    "        dir_tensor = torch.full((MAX_LEN, ),\n",
    "                                32,\n",
    "                                dtype=torch.uint8,\n",
    "                                device='cuda')\n",
    "        if rank == 0:\n",
    "            mmcv.mkdir_or_exist('.dist_test')\n",
    "            tmpdir = tempfile.mkdtemp(dir='.dist_test')\n",
    "            tmpdir = torch.tensor(\n",
    "                bytearray(tmpdir.encode()), dtype=torch.uint8, device='cuda')\n",
    "            dir_tensor[:len(tmpdir)] = tmpdir\n",
    "        dist.broadcast(dir_tensor, 0)\n",
    "        tmpdir = dir_tensor.cpu().numpy().tobytes().decode().rstrip()\n",
    "    else:\n",
    "        mmcv.mkdir_or_exist(tmpdir)\n",
    "    # dump the part result to the dir\n",
    "    mmcv.dump(result_part, osp.join(tmpdir, f'part_{rank}.pkl'))\n",
    "    dist.barrier()\n",
    "    # collect all parts\n",
    "    if rank != 0:\n",
    "        return None\n",
    "    else:\n",
    "        # load results of all parts from tmp dir\n",
    "        part_list = []\n",
    "        for i in range(world_size):\n",
    "            part_file = osp.join(tmpdir, f'part_{i}.pkl')\n",
    "            part_list.append(mmcv.load(part_file))\n",
    "        # sort the results\n",
    "        ordered_results = []\n",
    "        for res in zip(*part_list):\n",
    "            ordered_results.extend(list(res))\n",
    "        # the dataloader may pad some samples\n",
    "        ordered_results = ordered_results[:size]\n",
    "        # remove tmp dir\n",
    "        shutil.rmtree(tmpdir)\n",
    "        return ordered_results\n",
    "\n",
    "\n",
    "def collect_results_gpu(result_part, size):\n",
    "    rank, world_size = get_dist_info()\n",
    "    # dump result part to tensor with pickle\n",
    "    part_tensor = torch.tensor(\n",
    "        bytearray(pickle.dumps(result_part)), dtype=torch.uint8, device='cuda')\n",
    "    # gather all result part tensor shape\n",
    "    shape_tensor = torch.tensor(part_tensor.shape, device='cuda')\n",
    "    shape_list = [shape_tensor.clone() for _ in range(world_size)]\n",
    "    dist.all_gather(shape_list, shape_tensor)\n",
    "    # padding result part tensor to max length\n",
    "    shape_max = torch.tensor(shape_list).max()\n",
    "    part_send = torch.zeros(shape_max, dtype=torch.uint8, device='cuda')\n",
    "    part_send[:shape_tensor[0]] = part_tensor\n",
    "    part_recv_list = [\n",
    "        part_tensor.new_zeros(shape_max) for _ in range(world_size)\n",
    "    ]\n",
    "    # gather all result part\n",
    "    dist.all_gather(part_recv_list, part_send)\n",
    "\n",
    "    if rank == 0:\n",
    "        part_list = []\n",
    "        for recv, shape in zip(part_recv_list, shape_list):\n",
    "            part_list.append(\n",
    "                pickle.loads(recv[:shape[0]].cpu().numpy().tobytes()))\n",
    "        # sort the results\n",
    "        ordered_results = []\n",
    "        for res in zip(*part_list):\n",
    "            ordered_results.extend(list(res))\n",
    "        # the dataloader may pad some samples\n",
    "        ordered_results = ordered_results[:size]\n",
    "        return ordered_results\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}